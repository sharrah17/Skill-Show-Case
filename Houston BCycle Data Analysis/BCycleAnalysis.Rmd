---
title: "Houston BCycle Data Analysis"
author: "Sharrah Allen"
output:
  rmdformats::downcute:
    code_folding: show
    self_contained: true
    thumbnails: false
    lightbox: true
    downcute_theme: "chaos"
    
---

# Introduction

BCycle is a bicycle sharing company located in various cities across the U.S, one location being in Houston. Houston BCycle is operated by a nonprofit called Houston Bike Share. Houston BCycle's goal is to provide Houstonians with access to bikes for recreation, transportation, or work. Houston BCycle currently has 140 stations with both electric and standard bikes.

The purpose of this project is to put my data science and R skills to use by exploring and analyzing data collected and provided by Houston BCycle.

# Packages Used

Below are the main packages used in this notebook:

#### dplyr

`dplyr` is a package that adds functions used to easily manipulate data.

More information on `dplyr` can be found here: https://www.rdocumentation.org/packages/dplyr/versions/0.7.8

#### grid and gridExtra

For this project, `grid` and `gridExtra` were primarily used to arrange and combine plots onto one page.

More information on `grid` can be found here: https://cran.r-project.org/web/packages/gridExtra/vignettes/arrangeGrob.html

#### mapview

`mapview` is a package that provides functions used to create interactive maps.

More information on `mapview` can be found here: https://cran.r-project.org/web/packages/mapview/mapview.pdf

#### ggplot2

`ggplot2` is a package in R used to create data visualizations. 

More information on `ggplot2` can be found here: https://ggplot2.tidyverse.org/


#### ggthemr

`ggthemr` is a package that provides color schemes for plots made using ggplot2.

More information on `themer` can be found here: https://github.com/Mikata-Project/ggthemr

#### rmdformats

`rmdformats` is a package that provides additional themes and formats for Rmarkdown HTML output.

More information on `rmdformats` can be found here: https://github.com/juba/rmdformats



```{r warning=FALSE, include=FALSE}
# install.packages("tidyverse")
library(tidyverse)  
#used for visualizations
# install.packages("ggplot2") 
library(ggplot2)
# install.packages('nortest')
library(nortest)
# install.packages("car")
library(car)
#install.packages("grid")
#install.packages("gridExtra")
library(grid)      
library(gridExtra)
library(sf)
# for plotting
# library(lattice)
library(leafpop)
library(mapview)
# install.packages("reshape2")
library(reshape2)
# install.packages("kableExtra")
library(kableExtra)
library(DT)
library(knitr)
library(ggthemr)
ggthemr('dust')
```


# Data Preparation

Before I could explore and analyze the provided data, I had to load in, clean, and prepare various data sets.

## Primary Dataframe Preparation

The excel sheet provided by Houston BCycle was separated into different sheets for each month (January through August). I separated each sheet into a separate Comma Separated Values (CSV) file for each month. I then loaded each individual CSV file into RStudio. After loading the CSV file for each month, I added a column for the month that each data frame represented using `mutate()`. This was done so that I would be able to group by month later on when exploring the data. 

```{r}
#Load in CSV for each month:
Jan21 <- read.csv("jan21.csv")
Feb21 <- read.csv("feb21.csv")
March21 <- read.csv("march21.csv")
April21 <- read.csv("april21.csv")
May21 <-  read.csv("may21.csv")
June21 <-  read.csv("june21.csv")
July21 <-  read.csv("july21.csv")
Aug21 <- read.csv("august21.csv")


#Add column for month using mutate():
Jan21 <- Jan21 %>% mutate(Month = "January")
# Jan21

Feb21 <- Feb21 %>%  mutate(Month = "February")
# Feb21

March21 <- March21 %>%  mutate(Month = "March")
# March21

April21 <- April21 %>% mutate(Month = "April")
# April21

May21 <- May21 %>% mutate(Month = "May")
# May21

June21 <- June21 %>%  mutate(Month = "June")
# June21

July21 <- July21 %>% mutate(Month = "July")
# July21

Aug21 <- Aug21 %>% mutate(Month = "August")
# Aug21
```

After adding the extra column for month, I combined the data frames using `rbind()`.

```{r}
#Combine dataframes using rbind():
Jan21Aug21Data <- rbind(Jan21, Feb21, March21, April21, May21, June21, July21, Aug21)
# Jan21Aug21Data %>% head(10)
# Jan21Aug21Data %>% tail(10)
```

It is important to note that when preparing and gathering data for the map I created (mentioned below), I noticed that there were duplicate stations due to incorrect spelling. Some observations had stations where "Fifth Ward CRC *Lyons* & Waco" is spelled as "Fifth Ward CRC *Lycons* & Waco", while other observations had "Dallas & St *Emmanuel*" spelled as "Dallas & St *Emanuel*". To correct this, I decided to export the primary data frame I created to a CSV file so that I can correct the misspelled stations in Excel. Observations with checkout or return stations as "Fifth Ward CRC *Lycons* & Waco" were changed to "Fifth Ward CRC *Lyons* & Waco", and observations with checkout or return stations as "Dallas & St *Emanuel*" were changed to "Dallas & St *Emmanuel*". Even though Dallas & St Emmanuel is located along St Emanuel Street, I decided on the spelling "Dallas & St Emmanuel" since that is what the station is officially named. After fixing the station names, I imported the corrected primary data set back into RStudio. I then removed the observations where the column UserRole is "Maintenance" since I assume that these "trips" are actually for maintenance purposes conducted by Houston BCycle employees and that they are not real trips taken by users. This final data frame, **Jan21Aug21Data**, was used for data exploration.


```{r}
# Export primary data frame to an CSV file to correct incorrect spelling of station names in excel:
# write.csv(Jan21Aug21Data, "C:/Users/deish/OneDrive/Desktop/BCycle/correctThis.csv")


#Import corrected primary data set:
Jan21Aug21Data <- read.csv("correctThis.csv")
# Jan21Aug21Data %>% head(10)


#Remove maintenance:
Jan21Aug21Data <- Jan21Aug21Data %>% filter(UserRole != "Maintenance")
```


To finish, I selected the columns from the data frame that will be used for data analysis. These columns are as follows:

- UserID
- CheckoutKioskName
- ReturnKioskName
- DurationMins
- Distance
- EstimatedCarbonOffset
- EstimatedCaloriesBurned
- CheckoutDateLocal
- TripRouteCategory
- Month

```{r}
#Select specific columns:
Jan21Aug21Data <-  Jan21Aug21Data %>% select(c(UserId, CheckoutKioskName, ReturnKioskName, DurationMins, Distance, EstimatedCarbonOffset, EstimatedCaloriesBurned, CheckoutDateLocal, TripRouteCategory, Month))


#Turn months into leveled factors:
Jan21Aug21Data$Month <- factor(Jan21Aug21Data$Month, levels = c('January', 'February', 'March' ,'April', 'May', 'June', 'July', 'August'))
```

Below is  a snippet of the final primary data frame **Jan21Aug21Data** :

```{r}
#Use DT library to display data frame:
datatable(head(Jan21Aug21Data, 1000),
  options = list(scrollX = TRUE))
```



## Map Data Preparation

I decided that I wanted to look at the number of checkouts from each station on a map to explore where popular stations are located.

In order to map each station, I needed to find the coordinates for each station location. To begin, I took the primary data frame I created and created a new data frame containing all unique station names from the ReturnKioskName column. This data frame is called **SortedReturnKiosk**. I chose to use the ReturnKioskName column since it had one more station than the CheckoutKioskName column (as seen below using `length(unique())`). The extra station in the ReturnKioskName column is called "Stolen Bike Warehouse".


```{r}
#Use unique() on each column to see station names. Use length to see if the numbers match up, if they don't, then one of the columns has an extra station:
# length(unique(Jan21Aug21Data$CheckoutKioskName))
# length(unique(Jan21Aug21Data$ReturnKioskName))


#Find the extra station by comparing return and checkout kiosk name columns:
SortedReturnKiosk <- Jan21Aug21Data %>% select(ReturnKioskName) %>% arrange(ReturnKioskName)
# sorted_return

SortedCheckoutKiosk <- Jan21Aug21Data %>% select(CheckoutKioskName) %>% arrange(CheckoutKioskName)
# sorted_checkout


#Make data frame of unique values only:
SortedReturnKiosk <- unique(SortedReturnKiosk)
# SortedReturnKiosk
```

After creating the data frame **SortedReturnKiosk**, I exported the data to a CSV file so that I could edit the file in Excel and add latitude and longitude values for each station. 

```{r}
#Export dataframe with stations names to CSV so that I can add longitude and latitude manually in excel:
# write.csv(SortedReturnKiosk, "C:/Users/deish/OneDrive/Desktop/BCycle/station_names.csv")
```

It is important to note that when going through the excel sheet of station names, I noticed that there were duplicate observations due to incorrect spelling of station names. I spoke about how I corrected this earlier on in the document. It is also important to note that out of the 143 station names in the excel document, five of them are "stations" that do not have locations, nor do not appear in the BCycle phone application or website. These five stations are the following:

- Jury Assembly
- Houston B-cycle Warehouse
- Ready Bike Warehouse
- Customer Serive Virtual Dock
- Stolen Bike Warehouse

Since these "stations" did not have locations on the BCycle map located on both their website and phone application, these "stations" did not get assigned latitude or longitude values in the Excel sheet.
 
It is also important to mention that the data provided does not include two stations. BCycle has a total of 140 stations, but the data set provided only has 138 stations (with the five stations mentioned previously not included). The stations that are not in this data set provided are the Earl Henderson Park and Lamar and Bagby station. Below, I filter for both stations in both the CheckoutKioskName and ReturnKioskName column to show that these stations are not in the data set that was provided. All returned data frames have zero rows, indicating that these stations are not in the data set.


```{r}
#Filter for missing stations to check that they are not in the Checkout or Return kiosk name columns:
nrow(Jan21Aug21Data %>% filter(CheckoutKioskName == "Earl Henderson Park" | ReturnKioskName == "Earl Henderson Park"))

nrow(Jan21Aug21Data %>% filter(CheckoutKioskName == "Lamar & Bagby" | ReturnKioskName == "Lamar & Bagby"))
```

To find the coordinates of each station, I used a combination of Google Maps and the map feature in the BCycle phone application to approximately get the coordinates for each station by placing pins on Google Maps. After I documented the coordinates of each station, I imported the complete CSV with stations names and coordinates into RStudio. After importing it, I renamed the column to CheckoutKioskName so that I can join the data frame to another. Once I renamed the column, I found the number of trips/checkouts for each station in the CheckoutKioskName column by grouping by the CheckoutKioskName column and counting the number of observations in each group (or station). I then joined the two data frames by the CheckoutKioskName column. Lastly after joining, I removed the five "stations" with no coordinates to prevent errors when creating the map since those "stations" don't have coordinates. The final data frame **MapData** has longitude, latitude, and number of checkouts for each station. 

```{r echo=TRUE}
#import completed CSV with station names and coordinates:
MapData <- read.csv("station_namesMod.csv")


#Renamed column name to CheckoutKioskName for joining purposes:
MapData <- MapData %>% rename(CheckoutKioskName = ReturnKioskName)


#Found number of trips/checkouts for each station in CheckoutKioskName column:
TripCount <- Jan21Aug21Data %>% group_by(CheckoutKioskName) %>% 
  count() %>% 
  arrange(CheckoutKioskName) %>% 
  rename(NumCheckouts = n)
# TripCount

#Joined two dataframes by CheckoutKioskName
MapData <- left_join(MapData, TripCount)
# MapData


#Removed "stations" that have no longitude and latitude:
MapData <- MapData %>% filter(CheckoutKioskName != "Jury Assembly", CheckoutKioskName != "Houston B-cycle Warehouse", CheckoutKioskName != "Ready Bike Warehouse", CheckoutKioskName !="Customer Serive Virtual Dock", CheckoutKioskName != "Stolen Bike Warehouse")
```


Below is the **MapData** data frame:

```{r}
#MapData final data frame:
datatable(MapData)
```


## Regression Model Data Preparation

I decided that I wanted to look at the affect, if any, that temperature, day of the week, season, and holiday might have on the amount of bike rentals per day. To find out if there is an association, I decided to create a Multiple Linear Regression (MLR) model with regressors temperature, season, day of the week, and holiday and response variable as the count of daily bike rentals. Before I could create the model, I needed to prepare a data frame that has the values I previously mentioned. 

First, to get the amount of bike rentals per day, I took the primary data frame and grouped by the CheckoutDateLocal column and then used `count()` to count the number of observations in each group. I saved that to a new data frame and exported it to a CSV file so that I could add values for the regressors in Excel. 


```{r}
#Group by day and count to get count of  bike checkouts per day:
Regression <- Jan21Aug21Data %>% group_by(CheckoutDateLocal) %>% count()  
# regression %>% head(2)


#Write data frame to CSV:
# write.csv(Regression, "C:/Users/deish/OneDrive/Desktop/BCycle/addRegressors_.csv")
```

For temperature, I retrieved the average daily Fahrenheit temperature in Houston from the following website: https://www.weather.gov/. The average daily temperatures for each day were added to the Excel sheet.

```{r eval=FALSE, include=FALSE}
#Please ignore. This is here for my sake (so I will know how where exactly I got the data from)

# *https://www.weather.gov/ -> past weather -> select houston from map to get webpage https://www.weather.gov/wrh/Climate?wfo=hgx. selected "daily data for month"*
```


In Excel, I coded the values for regressors season, day of the week and holiday as numbers since they will serve as indicator variables. What the numbers mean is as follows:

#### Season
- 1: Winter (December 21, 2020 - March 19, 2021)
- 2: Spring (March 20, 2021	- June 20, 2021)
- 3: Summer (June 21 and after)

#### Day of the week:
- 1: Monday
- 2: Tuesday
- 3: Wednesday
- 4: Thursday
- 5: Friday
- 6: Saturday
- 7: Sunday

#### Holiday:
- 0: Not a holiday
- 1: Holiday

The holidays that I designated were New Year's Day, MLK Day, Easter, Earth Day, Cinco de Mayo, Mother's Day, Memorial Day, Juneteenth, Father's Day, and the Fourth of July. 


After adding the data for the regressors in Excel, I imported the finished CSV file into RStudio. I transformed the column data types of season, day of the week, and holiday to factors since they are indicator variables. Lastly, I removed the column CheckoutDateLocal since that will not be used in regression model. The final prepared data frame is called **RegData**.


```{r}
#Read in finished CSV with regression data:
NewDat <- read.csv("addRegressors.csv")
# new_Dat


#Use as.factor() since the columns below are indicator variables:
NewDat$Season <- as.factor(NewDat$Season)
NewDat$Day <- as.factor(NewDat$Day)
NewDat$Holiday <- as.factor(NewDat$Holiday)


#Remove columns that will not be used:
RegData = subset(NewDat, select = c(-CheckoutDateLocal, -Month))
```

Below is the **RegData** data frame:

```{r}
datatable(RegData)
```

Data preparation is now complete. I will now move forward with utilizing the prepared data frames for data exploration and analysis.


# Data Analysis

With the data prepared and cleaned, I can now start analyzing the data.


## Exploratory Analysis

Below are some numbers and averages for 2021:

```{r echo=FALSE}
include_graphics("C:/Users/deish/OneDrive/Desktop/StuffHere/BCycle/2021Numbers.png")
```

- The average number of calories burned after a trip is 214.03
- The average number of carbon offset after a trip is 5.09 pounds
- The average distance traveled in a trip is 5.36 miles
- The average trip duration is 54 minutes

```{r include=FALSE}
#Overall averages:
mean(Jan21Aug21Data$EstimatedCaloriesBurned)


mean(Jan21Aug21Data$EstimatedCarbonOffset)


mean(Jan21Aug21Data$Distance)


mean(Jan21Aug21Data$DurationMins)
```

```{r include=FALSE}
#Total trips thus far:
nrow(Jan21Aug21Data)

#Number of riders using unique on userId:
length(unique(Jan21Aug21Data$UserId)) 

#Total calories burned:
Jan21Aug21Data %>% select(EstimatedCaloriesBurned) %>% summarise(across(everything(), sum))

#Total carbon offset:
Jan21Aug21Data %>% select(EstimatedCarbonOffset) %>% summarise(across(everything(), sum))


#Total miles:
Jan21Aug21Data %>% select(Distance) %>% summarise(across(everything(), sum))


#Total ride time:
Jan21Aug21Data %>% select(DurationMins) %>% summarise(across(everything(), sum))
```


## In Depth Look at Trips

In this section, more information about trips will be uncovered.


Below is a visualization showing the amount of trips taken for each month. The plot suggests that the least amount of trips were taken in January and February, while the most amount of trips were taken in April, July, and March.


```{r}
#Plot data for number of trips per month:
PlotOneData <- Jan21Aug21Data %>% group_by(Month) %>% 
  count()
#PlotOneData


#Plot of number of trips per month:
ggplot(PlotOneData, aes(x = Month, y = n)) +
  geom_col(fill = "#D05034", alpha = .75)+
  ggtitle("Total Trips Taken by Month")+
  labs(y = "Number of Trips", x = "Month")
```

Next, to find out how much the number of trips fluctuated between each month, the percent change of total trips between each month was calculated. Below is a table showing the percent changes. 

```{r echo=FALSE}
#Monthly trip amount percent change:
#formula : (new-orginal)/(original * 100)


#jan - > feb
Feb <- (13653 - 21739) /(21739 * 100)

#feb -> march
March <- (27710	 - 13653)/ (13653 * 100)

#march -> arpil 
April <- (28011 - 27710)/ (27710 * 100)

#april -> may 
May <- (24079 - 28011)/(28011 * 100)
  
#may -> june
June <- (22871 - 24079)/ (24079 *  100)

#june - > july
July <- (27315 - 22871)/ (22871 * 100)

#july - > august
August <- (22333 - 27315)/ (27315 *100)
  
MonthlyTripPercentChange <- tribble(
  ~Month, ~NumberTrips, ~PercentChange,
  "January",21739, NA,
  "February",13653, Feb,
  "March", 27710, March,
  "April",28011, April,
  "May",24079, May,
  "June", 22871,June,
  "July", 27315,July,
  "August", 22333,August
)

datatable(MonthlyTripPercentChange)
```

Below are three plots showing the total carbon offset, calories burned, and miles traveled for each month. The graphs are all fairly the same, with February having the lowest counts of the three, and April and March having the highest counts of the three compared to other months. This makes sense as April and March are the two months with the most trips, while February has the least amount of trips. The three plots have a similar shape indicating that estimated calories burned and carbon offset is a linear function of miles traveled.


```{r include=FALSE}
Jan21Aug21Data %>% summarize(sum(Distance))

PlotThreeData <- Jan21Aug21Data %>% group_by(Month) %>%
  summarize(distance = sum(Distance)) %>%
  arrange(distance)
PlotThreeData


PlotFourData  <- Jan21Aug21Data %>% group_by(Month) %>% 
  summarize(caloriesburned = sum(EstimatedCaloriesBurned))

PlotFourData


PlotFiveData <- Jan21Aug21Data %>% group_by(Month) %>% 
  summarize(carbonoffset = sum(EstimatedCarbonOffset))

PlotFiveData 
```

```{r message=FALSE, warning=FALSE}
#Plot for miles traveled by month:
PlotThree<- ggplot(PlotThreeData, aes(x = Month, y = distance)) +
  geom_col(fill = "#D05034", alpha = .75)+
  ggtitle("Total Miles Traveled By Month")+
  labs(y = "Distance (miles)", x = "Month")


#Plot for calories burned by month
PlotFour <- ggplot(PlotFourData, aes(x = Month, y = caloriesburned)) +
  geom_col(fill = "#789C5B", alpha = .75)+
  ggtitle("Total Calories Burned By Month")+
  labs(y = "Calories Burned", x = "Month")


#Plot for carbon offset by month:
PlotFive <- ggplot(PlotFiveData, aes(x = Month, y = carbonoffset)) +
  geom_col(fill = "#825F53", alpha = .75)+
  ggtitle("Total Carbon Offset By Month")+
  labs(y = "Carbon Offset", x = "Month")

#Grid.arrange used to have plots displayed on one page:
grid.arrange(PlotThree, PlotFour, PlotFive)
```

Box plots of carbon offset, calories burned, and miles traveled by month are shown below. The three plots have similar distributions for each month indicating that estimated calories burned and carbon offset is a linear function of miles traveled. The box plots for each month also overlap. The overlap indicates that most of the data points are concentrated in similar ranges. This suggests that most trips for each month have similar values for miles traveled and, subsequently, carbon offset and calories burned. The box plots are skewed right with each month having one or more outliers.


```{r include=FALSE}
#averages by month
AvgCaloriesBurned <- Jan21Aug21Data %>% group_by(Month) %>% 
  summarize(AvgEstimatedCaloriesBurned = mean(EstimatedCaloriesBurned), MedEstimatedCaloriesBurned = median(EstimatedCaloriesBurned))


AvgCarbonOffset <- Jan21Aug21Data %>% group_by(Month) %>% 
  summarize(AvgEstimatedCarbonOffset = mean(EstimatedCarbonOffset), MedEstimatedCarbonOffset = median(EstimatedCarbonOffset))

AvgDistance <- Jan21Aug21Data %>% group_by(Month) %>% 
  summarize(AvgDistance = mean(Distance), MedDistance = median(Distance))


Avgs <- merge(AvgCaloriesBurned, AvgCarbonOffset)
Avgs <- merge(Avgs, AvgDistance) 
Avgs
```

```{r}
#Distance traveled by month box plot below:
DistanceMonthPlot <- ggplot(Jan21Aug21Data, aes(x = Month, y = Distance)) +
  geom_boxplot(fill = c("#D05034", "#789C5B", "#825F53","#D05034", "#789C5B", "#825F53","#D05034","#789C5B"), alpha  = 0.75, outlier.color = "#789C5B")+
   ggtitle("Distribution of Miles Taveled by Month")+
  labs(y = "Miles Traveled", x = "Month")+
  coord_flip()

DistanceMonthPlot
```

```{r echo=FALSE}
#Estimated calories burned by Month plot below:
CaloriesMonthPlot <- ggplot(Jan21Aug21Data, aes(x = Month, y = EstimatedCaloriesBurned)) +
  geom_boxplot(fill = c("#D05034", "#789C5B", "#825F53","#D05034", "#789C5B", "#825F53","#D05034","#789C5B"), alpha  = 0.75, outlier.color = "#789C5B")+
  ggtitle("Distribution of Calories Burned by Month")+
  labs(y = "Calories Burned", x = "Month")+
   coord_flip()
CaloriesMonthPlot


#Estimated carbon offset by Month plot below:
CarbonMonthPlot <- ggplot(Jan21Aug21Data, aes(x = Month, y = EstimatedCarbonOffset)) +
  geom_boxplot(fill = c("#D05034", "#789C5B", "#825F53","#D05034", "#789C5B", "#825F53","#D05034","#789C5B"), alpha  = 0.75, outlier.color = "#789C5B") +
  ggtitle("Distribution of Carbon Offest by Month")+
  labs(y = "Carbon Offset", x = "Month")+
   coord_flip()
CarbonMonthPlot

```


Most trips users take are round trips, meaning that they checkout and return the bike to the same station. In total, there have been:

```{r echo=FALSE}
include_graphics("C:/Users/deish/OneDrive/Desktop/StuffHere/BCycle/tripCategory.png")
```


```{r include=FALSE}
Jan21Aug21Data %>% group_by(TripRouteCategory)
# plot2_data
```

Next, we will take a look at the most popular routes taken by users. These routes happen to all be round trips. Below, the top ten routes are as follows:

- Sabine to Sabine: 12058 trips
- Eleanor Tinsley Park to Eleanor Tinsley Park: 11140 trips
- Centennial Gardens to Centennial Gardens: 6298 trips	
- Crawford Island to Crawford Island: 5924 trips
- Hermann Park Lake Plaza	to Hermann Park Lake Plaza: 5460 trips	
- La Branch & Lamar	to La Branch & Lamar:	4903 trips	
- Lamar & Crawford to	Lamar & Crawford: 3247 trips	
- Spotts Park	to Spotts Park:	2812 trips	
- Stude Park to Stude Park:	2491 trips	
- Alexander Deussen Park Boat Ramp/Parking Lot/Pavilion 4	to Alexander Deussen Park Boat Ramp/Parking Lot/Pavilion 4: 2196 trips

```{r include=FALSE}
#Find routes
TripRoutesMatrix <- table(Jan21Aug21Data$CheckoutKioskName, Jan21Aug21Data$ReturnKioskName)

#Use melt to turn matrix in DF:
TripRouteDF <- melt(TripRoutesMatrix)

#Arrange routes by descending order of checkouts:
TripRouteDF <- TripRouteDF %>% arrange(desc(value))

#Show top ten routes:
TripRouteDF %>% head(10)
```

Lastly, below are a few routes that have yet to be taken. The routes are as follows: 

- Montie Beach Park to West Gray & Baldwin
- 1919 Runnels to Westheimer & Waugh	
- Ben Taub Hospital to	UHD Jesse H. Jones Student Life Center	
- The Southmore	to Smith & Capitol	
- UT Research Park	to Woodland Park	

```{r include=FALSE}
#Show routes not used:
TripRouteDF %>% tail(4000)
```


## Map Exploration

Below, I create a map to explore where the most popular and least popular stations are located.

I used the **MapData** data frame that I prepared earlier on to create the map. The **MapData** data frame contains coordinates and total checkout counts for each station. Below, I map the stations using the coordinates. Each station's pin on the map is a circle where the size is related to the station's total number of checkouts. Clicking on the station pins on the map will give you the station name and "NumCheckouts" (the number of total checkouts for that station).

```{r message=FALSE, warning=FALSE}
#Use mapview to create interactive map:
StationLocations <- st_as_sf(MapData, coords = c("Longitude", "Latitude"), crs = 4326)
mapview(StationLocations, cex = "NumCheckouts", color ="#D05034",  col.regions= "#825F53", alpha = 0.5)
```


Based on the map, some of the most popular stations with the most checkouts are located near parks, trails, or attractions, such as Discover Green, for example. These stations are the following:

- Sabine Bridge: 15770 total checkouts
- Eleanor Tinsley Park: 14448	 total checkouts
- Crawford Island: 8569 total checkouts
- Centennial Gardens: 8551 total checkouts
- La Branch & Lamar:	7688 total checkouts
- Hermann Park Lake Plaza: 7352 total checkouts
- Lamar & Crawford: 5262 total checkouts
- Hermann Park/Rice U METROrail: 3544 total checkouts
- Spotts Park: 3523	total checkouts
- Stude Park: 3097 total checkouts


To conclude, after taking a look at the map, one recommendation for Houston BCycle would be to consider the importance of the Fonde Recreation Center station. The station seems to be about a 3-6 minute walk from one of the most popular stations in the data set, the Sabine Bridge station. The Fonde Recreation Center station has had a total of 1577 checkouts thus far, while the Sabine Bridge station has had a total of 15770 checkouts. It could be possible that the Fonde Recreation Center station has less total checkouts because most people prefer to go to the Sabine Bridge station instead. If this is the case, then it may be cost effective to not have the Fonde Recreation Center station since the two stations are fairly close to each other and the Sabine Bridge station is more preferred.

```{r eval=FALSE, include=FALSE}
#*When grouping by station name in the checkoutkioskname column to find the number of checkouts per station, deluxe theater was not calculated since it was not in the checkoutkiosk name column due to there being no checkouts from that station. However, when joining the data with the dataframe containing coordinates, the value for deluxe theater was filled to NA since deluze theater existed in the dataframe with coordinates.)*
```

```{r eval=FALSE, include=FALSE}
#Bottom five stations:
MapData %>% arrange(desc(NumCheckouts)) %>% tail(5)

```


## Regression Model Analysis

Does day of the week, holiday, temperature, or season have an affect on the number of bike rentals in a day? To find this out, I will create at MLR model to see if there is an association.

As mentioned previously, the categorical data are coded as numbers since they are indicator variables. Below are what the numbers mean for each variable:

#### Season
- 1: Winter (December 21, 2020 - March 19, 2021)
- 2: Spring (March 20, 2021	- June 20, 2021)
- 3: Summer (June 21 and after)

#### Day of the week:
- 1: Monday
- 2: Tuesday
- 3: Wednesday
- 4: Thursday
- 5: Friday
- 6: Saturday
- 7: Sunday

#### Holiday:
- 0: Not a holiday
- 1: Holiday


Below, I plot the individual regressors against the response variable, total daily bike rental. The categorical and continuous variables look to be associated with the response variable. As expected, bike rentals seems to increase during the warmer seasons and temperatures. The holiday plot suggests that holidays increase daily bike rentals on average. The weekday plot suggests that bike rental is higher on Sunday and Saturday while daily bike rental, on average, is *about* the same during the weekdays.

```{r}
#Box plot of daily bike rentals by season:
SeasonPlot <- ggplot(RegData, aes(x = Season, y = NumDailyCheckouts))+
  geom_boxplot(fill = c("#D05034", "#789C5B", "#825F53"), alpha  = 0.75, outlier.color = "#789C5B")+
  ggtitle('Daily Bike Rental by Season')+
  labs(y= "Daily Bike Rentals")


#Box plot of daily bike rentals by holiday:
HolidayPlot <- ggplot(RegData, aes(x = Holiday, y = NumDailyCheckouts))+
  geom_boxplot(fill = c("#D05034","#825F53"), alpha  = 0.75, outlier.color = "#789C5B")+
  ggtitle('Daily Bike Rental by Holiday')+
  labs(y= "Daily Bike Rentals")


#Box plot of daily bike rentals by day of week:
DayPlot <- ggplot(RegData, aes(x = Day, y = NumDailyCheckouts))+
  geom_boxplot(fill = c("#D05034", "#789C5B", "#825F53","#D05034", "#789C5B", "#825F53","#D05034"), alpha = 0.75, outlier.color = "#789C5B")+
  ggtitle('Daily Bike Rental by Day')+
  labs(y= "Daily Bike Rentals", x = "Day of The Week")


#Scatter plot of daily bike rentals by average daily temperature:
TempPlot <- ggplot(RegData, aes(x = AvgTemperature, y = NumDailyCheckouts))+
  geom_point(color = "#789C5B", alpha = 0.75)+
  ggtitle('Daily Bike Rental by Temperature')+
  labs(y= "Daily Bike Rentals", x = "Average Daily Temperature (F)")

#Grid.arrange used to have all plots on one page:
grid.arrange(SeasonPlot,HolidayPlot, DayPlot, TempPlot)
```

Below, a MLR model is fit. Using the full model, the Cook's distance plot show distances all below one for all observations. This suggests that there is no overtly influential points in the MLR model.

```{r message=FALSE, warning=FALSE}
#Create linear model from data
RegModel <- lm(formula = NumDailyCheckouts ~ ., data = RegData) 


#Cooks distance plot to check for influential points:
plot(RegModel, which = 4, col = "#825F53")
```

The summary table for the MLR model below shows the model has a F0 of 13.99 with a p value less than 0.05. This suggests that the regression model is significant, and that there is a linear relationship between the response variable and any of the regressors. The summary table also shows that regressors AvgTemperature and two levels of indicator variable Day exhibit a significant affect on, or contribution to, the model since the p values are less than 0.05. 


Below, the Anova table suggests that overall each regressor has a significant contribution to the model at a significance level of 0.05. 

```{r}
#Summary table:
summary(RegModel)


#Anova table:
anova(RegModel) 
```

To check model adequacy, a normal QQ plot for the model residuals and a residuals vs. fitted plot are seen below. Looking at the Normal QQ plot alone, the residuals look *more or less* normal. However, the Shapiro and Anderson-Darling normality tests for the model suggests that the residuals are not normal. This suggests that the model may violate the regression model assumption of normally distributed residuals. Transformations were done in an attempt to remedy the issue, but none of the transformations seemed to work. Although the QQ plot for the model isn't entirely perfect, the residuals do not deviate *too* awfully from the normal fit line. So, the residuals could possibly meet some threshold for normality.

```{r message=FALSE, warning=FALSE}
#Normal QQ plot for model residuals:
plot(RegModel, which  = 2, col = "#825F53")


#Shapiro test for normality of residuals:
shapiro.test(RegModel$res)


#Anderson D. test for normality of residuals:
ad.test(RegModel$res)
```

```{r eval=FALSE, include=FALSE}
#transformations of the response did not seem to fix violations of assumption

#log of response
LogTransform <- lm(formula = log(NumDailyCheckouts) ~ ., data = RegData) 

anova(LogTransform )

summary(LogTransform )

plot(LogTransform , which = 1)

#normality of residuals
plot2 <- plot(LogTransform , which  = 2)

#made plots worse?



#reciprocal of response
RecipTransform <- lm(formula = 1/(NumDailyCheckouts) ~ ., data = RegData) 

anova(RecipTransform)

summary(RecipTransform)

plot(RecipTransform, which = 1)

#normality of residuals
plot2 <- plot(RecipTransform, which  = 2)

#normality of residuals test
shapiro.test(RecipTransform$res)
ad.test(RecipTransform$res)

dwtest(RecipTransform, alternative="two.sided")

#did not fix error violations

#####################################################################################


#reciprocal square root of response

SqrtRecTransform <- lm(formula = 1/(sqrt(NumDailyCheckouts))~ ., data = RegData) 

anova(SqrtRecTransform)

summary(SqrtRecTransform)

plot(SqrtRecTransform, which = 1)

#normality of residuals
plot2 <- plot(SqrtRecTransform, which  = 2)

#normality of residuals test
shapiro.test(SqrtRecTransform$res)
ad.test(SqrtRecTransform$res)

dwtest(SqrtRecTransform, alternative="two.sided")

#did not work


######################################################################################

#square root of response
SqrtTransform <- lm(formula = sqrt(NumDailyCheckouts) ~ ., data = RegData) 

anova(SqrtTransform)

summary(SqrtTransform)

plot(SqrtTransform, which = 1)

#normality of residuals
plot2 <- plot(SqrtTransform, which  = 2)

#normality of residuals test
shapiro.test(SqrtTransform$res)
ad.test(SqrtTransform$res)

dwtest(SqrtTransform, alternative="two.sided")

#did not fix it
```


The residuals vs. fitted plot below does not have an observable shape or pattern, and the scatters points seem to be scattered randomly around the x-axis. This suggests that the model meets the regression model assumptions of constant variance and independence of errors.

```{r}
#Residuals v. fitted plot:
plot(RegModel, which = 1, col ="#825F53") 
```

Lastly, the VIF values for each regressor are less than 5, so multicollinearity, which can inflate the variance of the coefficients, is likely not an issue.

```{r}
#Check for multicollinearity using vif():
vif(RegModel)
```

Overall, the fitted regression model suggests that there is an linear association between the response variable (count of daily bike rentals) and the regressors (holiday, day of the week, season, and average daily temperature).The magnitudes and signs of the coefficients in the fitted model appear to be in line with expectations. As the average daily temperature increases, so do daily bike rentals. This makes sense as more people may be more inclined to travel/ride bikes in warmer temperatures compared to cooler temperatures. When it is around the weekend, bike rentals are at the highest, while bike rentals are impacted negatively when it is Monday-Friday. This could be due to the fact that more people are free to bike ride on the weekends due to having fewer obligations, such as work or school. If Houston BCycle wanted to increase the amount of daily bike rentals during the weekdays, they could perhaps offer an incentive to increase weekday bike rental counts, such as offering lower fare to users who use BCycle bikes to commute to work or school throughout the week, or perhaps some other award. Lastly, when it is a holiday, bike rentals seem to increase. This could be due to people being more inclined to participate in outdoor activities during some of the holidays, such as Memorial Day or July 4th.

Although the model mildly violates an assumption, it should be sufficient enough to make assumptions about the data. Additional collection of data could be added to the model to attempt to make it more adequate (currently the model was fit using a data set that has 243 observations). Additional regressors could also be added to the model, such as weather or wind speed, to further study what other factors influence the amount of daily bike rentals for Houston BCycle. 


# Future Work and Analysis

Overall, I was able to produce simple visualizations and create a regression model to provide insight about the data. Moving forward, since it is now 2022 it would be a good idea to redo what was done in this notebook with data from September through December added. With a complete year worth of data, 2021 can be compared to previous years to take a look at Houston BCycle's growth within the past year. 
 
It could also be beneficial to look at user data for the past year, and other years as well, to understand what groups of people, if any, utilize Houston BCycle the most. Doing so could help Houston BCycle understand how to maintain or attract certain groups of people through the use of marketing strategies or incentives.

Lastly, when analyzing the data, I noticed that there are a number of trips that lasted fewer than three or two minutes. These "trips" could be caused by people checking out a bike and soon returning the bike after not using it. Some of these trips with very short durations do have very small values for estimated carbon offset and calories burned. If these observations with very small durations aren't actually trips, then there are observations that contribute to calculated totals even though they are not trips. It may be worth it to decide on a way to distinguish between real trips and ones that aren't to remove the observations that aren't necessarily trips. This is to ensure that accurate totals are being calculated for estimated carbon offset and calories burned.


# Bibliography

- Sadler, Jesse. “Geocoding with R.” Jesse Sadler , 13 Oct. 2017, https://www.jessesadler.com/post/geocoding-with-r/. 

- Appelhans, Tim, et al. “Mapview Advanced Controls.” r-Spatial Mapview, https://r-spatial.github.io/mapview/articles/articles/mapview_02-advanced.html. 

- Shane. “Shane Lynn - Ggthemr.” Shane Lynn, https://www.shanelynn.ie/themes-and-colours-for-r-ggplots-with-ggthemr/. 

- Barnier, Julien, and HyoJin Song. “HTML Output Formats for RMARKDOWN Documents.” GitHub, 8 July 2014, https://github.com/juba/rmdformats. 

- Xie, Yihui. Bookdown: Authoring Books and Technical Publications with R Markdown. CRC Press, Taylor &amp; Francis Group, 2017. 

- Wickham, Hadley, and Garrett Grolemund. R For Data Science: Import, Tidy, Transform, Visualize and Model Data. O'Reilly, 2017. 
